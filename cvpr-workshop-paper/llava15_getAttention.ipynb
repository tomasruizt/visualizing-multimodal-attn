{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n",
    "device = \"cuda\"\n",
    "model_id = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_utils import load_image\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\")\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What color is the frisbee?\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"The frisbee is\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=False)\n",
    "prompt = prompt[:-1]  # remove a whitespace too much\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image = load_image(\"../imgs/frisbee.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(\n",
    "    device, torch.float16\n",
    ")\n",
    "inputs.input_ids.shape, inputs.pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.decode(inputs.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "str_toks = processor.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "n_img_toks = sum(tok == \"<image>\" for tok in str_toks)\n",
    "patches_per_side = float(np.sqrt(n_img_toks))\n",
    "print(\"num of img tokens\", n_img_toks)\n",
    "print(\"patches: \", (patches_per_side, patches_per_side))\n",
    "print(\"text tokens: \", [tok for tok in str_toks if tok != \"<image>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert pixel values to numpy array\n",
    "image_array = inputs.pixel_values[0].cpu().numpy().transpose(1, 2, 0).astype(np.float32)\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(image_array)\n",
    "plt.axis(\"off\")  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
