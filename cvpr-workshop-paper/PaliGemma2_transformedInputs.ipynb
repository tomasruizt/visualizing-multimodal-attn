{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from transformers import (\n",
    "    PaliGemmaProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    "    Gemma2Model,\n",
    "    Gemma2ForCausalLM,\n",
    ")\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_grad_enabled(False)  # avoid blowing up mem\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/paligemma2-3b-pt-224\"\n",
    "model = (\n",
    "    PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "        model_id, torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    .to(device)\n",
    "    .eval()\n",
    ")\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"imgs/frisbee.jpg\"\n",
    "image = Image.open(img_path)\n",
    "text = \"<image>Answer en what is the frisbee's color?\"\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**inputs, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute unnormalized importances for all tokens in layer 25\n",
    "all_importances = []\n",
    "all_fx_norms = []\n",
    "for layer in range(26):\n",
    "    layer_fx_norms = torch.load(f\"fx_norms/layer{layer}.pt\")\n",
    "    all_fx_norms.append(layer_fx_norms)\n",
    "    layer_attns = output.attentions[layer][0, :, :, :]  # all heads, all tokens\n",
    "    layer_imps = layer_fx_norms.unsqueeze(1) * layer_attns\n",
    "    all_importances.append(layer_imps)\n",
    "\n",
    "all_fx_norms = torch.stack(all_fx_norms).float()\n",
    "all_importances = torch.stack(all_importances).float()\n",
    "assert all_importances.shape == (26, n_heads := 8, 269, 269)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_imp = all_importances / all_importances.sum(dim=3, keepdim=True)\n",
    "print(normalized_imp.shape)\n",
    "assert torch.allclose(\n",
    "    normalized_imp[0, 0, 10, :].sum(), torch.tensor(1.0)\n",
    ")  # 0th batch, 0th head,10th token (should be true for any token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import dump_attn\n",
    "from getAttentionLib import get_img_grid_sizes\n",
    "\n",
    "\n",
    "token_strings = processor.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "assert token_strings[-1] == \"\\n\"\n",
    "token_strings[-1] = \"\\\\n\"\n",
    "_, grid_side_len = get_img_grid_sizes(model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [0, 15, 25]\n",
    "for layer in layers:\n",
    "    imp = all_importances[layer]\n",
    "    dump_attn(\n",
    "        attn_weights=imp[None, :],\n",
    "        layer_idx=layer,\n",
    "        name=\"PaliGemma2_transformedInputs\",\n",
    "        tokens=token_strings,\n",
    "        img_path=img_path,\n",
    "        grid_side_len=grid_side_len,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import multi_dot\n",
    "from getAttentionLib import (\n",
    "    compute_attn_sums,\n",
    "    compute_mult_attn_sums,\n",
    "    plot_attn_sums,\n",
    "    plot_mult_attn_sums,\n",
    "    plot_region_attn_progression,\n",
    ")\n",
    "\n",
    "n_img_tokens = grid_side_len**2\n",
    "imp_sums = torch.stack(\n",
    "    [\n",
    "        compute_attn_sums(imps, n_img_tokens=n_img_tokens)\n",
    "        for imps in normalized_imp[[0, 15, 25]]\n",
    "    ]\n",
    ")\n",
    "plot_mult_attn_sums(\n",
    "    None, None, layers=layers, mult_attn_sums=imp_sums, n_img_tokens=n_img_tokens\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_sums = []\n",
    "for layer_attns in torch.stack(output.attentions)[torch.tensor([0, 15, 25]), 0]:\n",
    "    attn_sums.append(compute_attn_sums(layer_attns, n_img_tokens=n_img_tokens).float())\n",
    "attn_sums = torch.stack(attn_sums)\n",
    "plot_mult_attn_sums(\n",
    "    None, None, layers=layers, mult_attn_sums=attn_sums, n_img_tokens=n_img_tokens\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = imp_sums - attn_sums\n",
    "plot_mult_attn_sums(\n",
    "    None,\n",
    "    None,\n",
    "    layers=layers,\n",
    "    mult_attn_sums=diffs,\n",
    "    n_img_tokens=n_img_tokens,\n",
    "    cmap=\"bwr\",\n",
    "    vmin=-0.4,\n",
    "    vmax=0.4,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final token in the final layer attends on average (over all heads) 0.336 to the <bos> token\n",
    "assert output.attentions[-1][0, :, -1, n_img_tokens].mean() == 0.336  # success\n",
    "# the final token in the final layers attens on average (over all heads) 0.175 to itself\n",
    "assert output.attentions[-1][0, :, -1, -1].mean() == 0.175  # success\n",
    "\n",
    "# The fx norm of the <bos> token in the final layer and that of the final token should be within 50% of each other\n",
    "# because their importances are very similar to their attention values\n",
    "l25_fx_norms = torch.load(f\"fx_norms/layer25.pt\")\n",
    "l25_fx_norms[:, n_img_tokens], l25_fx_norms[:, -1]  # does not hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute unnormalized importances in the final layer with destination token=final token\n",
    "print(l25_fx_norms.shape)\n",
    "l25_dest_last_tkn_attns = output.attentions[-1][\n",
    "    0, :, -1, :\n",
    "]  # last layer, 0th batch, all heads, dest=last token, src=all tokens\n",
    "print(l25_dest_last_tkn_attns.shape)\n",
    "assert (l25_dest_last_tkn_attns.sum(dim=1) == 1.0).all()  # equals 1.0 for all heads\n",
    "l25_imps = (l25_fx_norms * l25_dest_last_tkn_attns).sum(dim=0)\n",
    "assert len(l25_imps) == 269\n",
    "l25_imps[n_img_tokens], l25_imps[-1]  # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now they are close\n",
    "print(l25_imps[n_img_tokens], all_importances[-1, :, -1, n_img_tokens].sum())\n",
    "print(l25_imps[-1], all_importances[-1, :, -1, -1].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the f(x) norms develop over the layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import aggregate_layer_norms, plot_fx_norms_progressions\n",
    "\n",
    "\n",
    "max_norms, avg_norms = aggregate_layer_norms(all_fx_norms, n_img_tokens)\n",
    "plot_fx_norms_progressions(max_norms, avg_norms, sharey=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How different are f(x) norms for image tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dispersion coefficient (coefficient of variation) for each layer\n",
    "# This measures the relative variability of fx norms within each layer\n",
    "layer_means = all_fx_norms.sum(dim=1).mean(dim=1)\n",
    "layer_stds = all_fx_norms.sum(dim=1).std(dim=1)\n",
    "dispersion_coefficients = layer_stds / layer_means\n",
    "dispersion_coefficients.min(), dispersion_coefficients.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are Max-Norm Img Tokens also the Most Important Causal Ones?\n",
    "\n",
    "The answer seems to be no. There most important causal image tokens are centered on the frisbee's text.\n",
    "The Max-Norm img tokens follow no clear pattern. The importance of image tokens (f(x) norm * attention) seem to be slightly correlated with the causally important ones on the frisbee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_strings = processor.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "assert token_strings[-1] == \"\\n\"\n",
    "token_strings[-1] = \"\\\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from getAttentionLib import (\n",
    "    maxpool_img_tokens,\n",
    "    avgpool_img_tokens,\n",
    "    plot_img_and_text_probs_side_by_side,\n",
    "    plot_img_probs,\n",
    "    plot_pooled_probs_plotly,\n",
    ")\n",
    "from getAttentionLib import plot_pooled_probs_plt\n",
    "from getAttentionLib import plot_and_browse_img_token_in_probs\n",
    "\n",
    "frisbee2_purple_probs = torch.load(\"purple_probs_of_frisbee2_img.pt\")\n",
    "# frisbee2_pooled_purple_probs = avgpool_img_tokens(frisbee2_purple_probs)\n",
    "# plot_pooled_probs_plotly(\n",
    "#     frisbee2_pooled_purple_probs, token_strings, healthy_response_tok_name=\"purple\"\n",
    "# ).show()\n",
    "plot_img_and_text_probs_side_by_side(frisbee2_purple_probs, n_img_tokens=256).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_and_browse_img_token_in_probs(frisbee2_purple_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_fx_norms = all_fx_norms.mean(dim=1).cpu()\n",
    "plot_and_browse_img_token_in_probs(probs=mean_fx_norms, cmax=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how important are the img tokens for the final token?\n",
    "imp_for_last_tok = normalized_imp.mean(dim=1)[:, -1, :].cpu()\n",
    "plot_and_browse_img_token_in_probs(probs=imp_for_last_tok, cmax=0.05)\n",
    "# all_importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention only \"importances\" for the last token\n",
    "# sum over all heads, 0th batch, dest_token=last token, all src tokens\n",
    "lastlayer_lasttok_attn = (\n",
    "    torch.stack(output.attentions).float().sum(dim=2)[:, 0, -1, :].cpu()\n",
    ")\n",
    "plot_and_browse_img_token_in_probs(probs=lastlayer_lasttok_attn, cmax=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from correlation_analysis import (\n",
    "    plot_correlation_in_midlayers,\n",
    "    plot_correlation_progression,\n",
    ")\n",
    "\n",
    "\n",
    "_ = plot_correlation_progression(\n",
    "    imp_for_last_tok, frisbee2_purple_probs, p_threshold=0.001\n",
    ")\n",
    "_ = plot_correlation_progression(\n",
    "    mean_fx_norms, frisbee2_purple_probs, p_threshold=0.001\n",
    ")\n",
    "# plot_correlation_in_midlayers(mean_fx_norms, frisbee2_purple_probs, start_layer=8, end_layer=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from correlation_analysis import plot_correlation_in_midlayers\n",
    "# plot_correlation_in_midlayers(imp_for_last_tok, frisbee2_purple_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_correlation_progression(\n",
    "    lastlayer_lasttok_attn, frisbee2_purple_probs, p_threshold=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Log Prob Increase by Token\n",
    "This metrics looks in each layer at the change in log prob for the correct answer.\n",
    "The comparison is between the hidden state before and after the multi-head attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a hook in each layer that has saves the input & the module\n",
    "from getAttentionLib import paligemma_merge_text_and_image\n",
    "\n",
    "inputs_embeds = paligemma_merge_text_and_image(model, inputs)\n",
    "outputs = model(\n",
    "    inputs_embeds=inputs_embeds, output_attentions=True, output_hidden_states=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = torch.stack(outputs.hidden_states)\n",
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute base log probs for correct answer\n",
    "purple_token = 34999\n",
    "assert processor.tokenizer.decode(purple_token) == \"purple\"\n",
    "\n",
    "hidden_states_T = hidden_states[:, 0, -1, :]\n",
    "purple_log_probs = model.language_model.lm_head(hidden_states_T).softmax(dim=1)[\n",
    "    :, purple_token\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(range(1, len(purple_log_probs) + 1), purple_log_probs.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.language_model.model.layers[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
