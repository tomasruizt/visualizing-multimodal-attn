{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Yarn package repository\n",
    "!curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -\n",
    "!echo \"deb https://dl.yarnpkg.com/debian/ stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list\n",
    "\n",
    "!export NVM_DIR=\"$HOME/.nvm\"\n",
    "![ -s \"$NVM_DIR/nvm.sh\" ] && \\. \"$NVM_DIR/nvm.sh\"\n",
    "!nvm install 16\n",
    "# Install Yarn\n",
    "!sudo apt update\n",
    "!sudo apt install yarn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install uv\n",
    "!uv pip install -r requirements.txt\n",
    "!uv pip install 'accelerate>=0.26.0'\n",
    "!uv pip install torch\n",
    "!uv pip install flash-attn==2.7.4.post1 --no-build-isolation\n",
    "!./install_circuitsvis.sh\n",
    "# please install yarn as per README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "torch.set_grad_enabled(False)  # avoid blowing up mem\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "import torch\n",
    "DEVICE = \"cuda\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\")\n",
    "model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceTB/SmolVLM-Instruct\",\n",
    "                                                torch_dtype=torch.bfloat16,\n",
    "                                                attn_implementation=\"eager\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers.image_utils import load_image\n",
    "\n",
    "img_path = \"imgs/frisbee.jpg\"\n",
    "image = load_image(img_path)\n",
    "plt.axis(\"off\")\n",
    "_ = plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import get_response, get_attention, dump_attn, get_img_grid_sizes, plot_mult_attn_sums\n",
    "\n",
    "text = \"<image>What color is the frisbee?\"\n",
    "\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(model.device)\n",
    "inputs.pixel_values = inputs.pixel_values[:, :1, :, :, :]\n",
    "print(inputs.pixel_values.shape)\n",
    "print(inputs.keys())\n",
    "print(inputs.input_ids)\n",
    "input_tokens = processor.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "response: str = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print('inputs_tokens')\n",
    "print(input_tokens)\n",
    "print(outputs)\n",
    "\n",
    "print('\\nResponse:')\n",
    "print(response)\n",
    "\n",
    "\n",
    "# Prepare inputs\n",
    "# inputs_tokens, response = get_response(model, processor, prompt, image)\n",
    "# print(inputs_tokens)\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"<image>What color is the frisbee?\"\n",
    "# inputs = processor(text=text, images=image, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# # Check dimensions of inputs\n",
    "# print(\"Input IDs shape:\", inputs.input_ids.shape, inputs.input_ids[:13])\n",
    "# print(\"Image embeddings shape:\", inputs.image_embeds.shape if hasattr(inputs, 'image_embeds') else \"No direct image embeddings\")\n",
    "\n",
    "# # Get attention from first layer to see actual dimensions\n",
    "# outputs = model(input_ids=inputs.input_ids, image=image, output_attentions=True)\n",
    "# first_layer_attention = outputs.attentions[0]\n",
    "# print(\"\\nAttention shape:\", first_layer_attention.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Multimodal Attention Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Region-wise Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_mult_attn_sums(model, inputs, layers=[0, 10, 20])\n",
    "fig.savefig(\"imgs/blockwise-attn-sums-frisbee.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Numbers on VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from getAttentionLib import compute_mult_attn_sums, load_vqa_ds, plot_images_grid\n",
    "\n",
    "n_vqa_samples = 15 # 1000\n",
    "ds = load_vqa_ds(split=\"train\")\n",
    "\n",
    "layers = [0, 10, 20]\n",
    "attens_tensor = []\n",
    "responses = []\n",
    "imgs = []\n",
    "seen_imgs = set()\n",
    "pbar = tqdm.tqdm(total=n_vqa_samples)\n",
    "for row in ds:\n",
    "    if len(imgs) >= n_vqa_samples:\n",
    "        break\n",
    "\n",
    "    if row[\"image_id\"] in seen_imgs:\n",
    "        continue\n",
    "    seen_imgs.add(row[\"image_id\"])\n",
    "    print(row['question'])\n",
    "    text = f\"<image>{row['question']}\"\n",
    "    try:\n",
    "        inputs = processor(text=text, images=row[\"image\"], return_tensors=\"pt\").to(model.device)\n",
    "    except ValueError as e: # Unsupported number of image dimensions: 2\n",
    "        if 'Unsupported number' in str(e):\n",
    "            continue\n",
    "\n",
    "    response = get_response(model, processor, text, row[\"image\"])[1]\n",
    "    #responses.append(response.replace(\"\\n\", \" A: \").replace(\"Answer en\", \"Q:\"))\n",
    "    question = text.strip('<image>')\n",
    "    responses.append(question + '|' + response.split(question)[1])\n",
    "    \n",
    "    imgs.append(row[\"image\"])\n",
    "    \n",
    "    mult_attn_sums = compute_mult_attn_sums(model, inputs, layers=layers)\n",
    "    attens_tensor.append(torch.stack(mult_attn_sums))\n",
    "    \n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "    \n",
    "stacked_attens = torch.stack(attens_tensor)\n",
    "assert stacked_attens.shape == (n_vqa_samples, len(layers), 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = stacked_attens.mean(dim=0)\n",
    "assert means.shape == (len(layers), 3, 3)\n",
    "stds = stacked_attens.std(dim=0)\n",
    "assert stds.shape == (len(layers), 3, 3)\n",
    "fig = plot_mult_attn_sums(None, None,layers=layers, mult_attn_sums=means, stds=stds)\n",
    "fig.savefig(\"imgs/blockwise-attn-sums-vqa1000.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show VQA Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def process_response(r: str) -> str:\n",
    "    q, a = r.split('|')\n",
    "    first_response = a.split('.')[0].split('?')[0]\n",
    "    chars_per_line = 20\n",
    "    \n",
    "    # Wrap text to fit within subplot\n",
    "    a = textwrap.fill(first_response, width=chars_per_line)\n",
    "    return f\"Q: {q}\\nA: {a}\"\n",
    "\n",
    "process_response(responses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_responses = [process_response(r) for r in responses]\n",
    "fig = plot_images_grid(imgs[:15], proc_responses[:15], nrows=3, ncols=5, figsize=(10, 6))\n",
    "fig.savefig(\"imgs/vqa-grid-of-img-question-answer.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
