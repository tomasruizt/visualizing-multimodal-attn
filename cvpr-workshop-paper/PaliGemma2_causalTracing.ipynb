{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from transformers import (\n",
    "    PaliGemmaProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    "    Gemma2Model,\n",
    "    Gemma2ForCausalLM,\n",
    ")\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_grad_enabled(False)  # avoid blowing up mem\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/paligemma2-3b-pt-224\"\n",
    "model = (\n",
    "    PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "        model_id, torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    .to(device)\n",
    "    .eval()\n",
    ")\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"imgs/frisbee.jpg\"\n",
    "image = Image.open(img_path)\n",
    "plt.axis(\"off\")\n",
    "_ = plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(inputs_tokens) = 269\n",
      "['<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<image>', '<bos>', 'Answer', ' en', ' what', ' is', ' the', ' fris', 'bee', \"'\", 's', ' color', '?', '\\\\n']\n",
      "Answer en what is the frisbee's color?\n",
      "purple\n"
     ]
    }
   ],
   "source": [
    "from getAttentionLib import get_response\n",
    "\n",
    "\n",
    "text = \"<image>Answer en what is the frisbee's color?\"\n",
    "n_img_tokens = 256\n",
    "inputs_tokens, response = get_response(model, processor, text, image)\n",
    "inputs_tokens[-1] = \"\\\\n\"  # to print it nicely\n",
    "print(\"len(inputs_tokens) =\", len(inputs_tokens))\n",
    "print(inputs_tokens)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(model.device)\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import (\n",
    "    get_decoder_layer_outputs,\n",
    "    paligemma_merge_text_and_image,\n",
    "    gaussian_noising,\n",
    ")\n",
    "\n",
    "healthy_embeds = paligemma_merge_text_and_image(model, inputs)\n",
    "healthy_activations, healthy_outputs = get_decoder_layer_outputs(model, healthy_embeds)\n",
    "print(processor.decode(healthy_outputs.logits[0, -1, :].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_embeds = gaussian_noising(healthy_embeds, num_img_tokens=256)\n",
    "noisy_activations, noisy_outputs = get_decoder_layer_outputs(model, unhealthy_embeds)\n",
    "print(processor.decode(noisy_outputs.logits[0, -1, :].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import tok_prob\n",
    "\n",
    "purple_token = 34999\n",
    "assert processor.tokenizer.decode(purple_token) == \"purple\"\n",
    "assert noisy_activations.shape == healthy_activations.shape\n",
    "assert (\n",
    "    tok_prob(healthy_outputs, purple_token)\n",
    "    == model(**inputs).logits[0, -1, :].softmax(dim=-1).max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import ActivationPatchingResult, patch_all_activations\n",
    "\n",
    "\n",
    "patching_result: ActivationPatchingResult = patch_all_activations(\n",
    "    model,\n",
    "    healthy_activations,\n",
    "    unhealthy_embeds,\n",
    "    healthy_response_tok_idx=purple_token,\n",
    "    unhealthy_response_tok_idx=purple_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "patching_result.save(\n",
    "    Path(\"patching_result\"),\n",
    "    health_response_tok=\"purple\",\n",
    "    unhealthy_response_tok=\"purple\",\n",
    "    corruption_type=\"gaussian_noising\",\n",
    "    corruption_img_alias=None,\n",
    "    healthy_img_alias=img_path,\n",
    "    prompt=text,\n",
    ")\n",
    "del patching_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from getAttentionLib import ActivationPatchingResult\n",
    "\n",
    "\n",
    "patching_result = ActivationPatchingResult.load(Path(\"patching_result\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "purple_probs = patching_result.healthy_tok_response_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does the `<bos>` token get attention?\n",
    "As we can see below, the noisy input embeddings have a significantly different attention pattern than the healthy image input. \n",
    "Almost all attention goes in the image tokens. Almost no attention is allocated to the the `<bos>` token, which explains why it cannot influence the output, even when restored.\n",
    "This suggests that noise input data disrupts the learned attention patterns. To preserve the attention patterns, we need a different approach to corrput the input image.\n",
    "Using a different, unrelated image might work better, because it will preserve the attention patterns, as shown in VQA example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import maxpool_img_tokens, plot_pooled_probs_plotly\n",
    "\n",
    "plot_pooled_probs_plotly(\n",
    "    maxpool_img_tokens(purple_probs, n_img_tokens=n_img_tokens),\n",
    "    inputs_tokens,\n",
    "    healthy_response_tok_name=\"purple\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import plot_img_and_text_probs_side_by_side\n",
    "\n",
    "\n",
    "plot_img_and_text_probs_side_by_side(purple_probs, n_img_tokens, inputs_tokens).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purple_probs.max(dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import (\n",
    "    compute_mult_attn_sums,\n",
    "    plot_region_attn_progression,\n",
    "    plot_mult_attn_sums,\n",
    ")\n",
    "\n",
    "mult_attn_sums = compute_mult_attn_sums(\n",
    "    model,\n",
    "    {\"inputs_embeds\": unhealthy_embeds},\n",
    "    layers=list(range(len(model.language_model.model.layers))),\n",
    "    n_img_tokens=n_img_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_mult_attn_sums(\n",
    "#     None, None, layers=[0, 15, 25], mult_attn_sums=mult_attn_sums\n",
    "# ).show()\n",
    "# plot_region_attn_progression(None, None, mult_attn_sums=mult_attn_sums).show()\n",
    "plot_mult_attn_sums(\n",
    "    None,\n",
    "    None,\n",
    "    layers=[0, 15, 25],\n",
    "    mult_attn_sums=[mult_attn_sums[e] for e in [0, 15, 25]],\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get standard deviations using VQA samples\n",
    "from getAttentionLib import compute_mult_attn_sums_over_noisy_vqa\n",
    "\n",
    "n_vqa_samples = 100\n",
    "stacked_attn_sums = compute_mult_attn_sums_over_noisy_vqa(\n",
    "    model, processor, n_vqa_samples=n_vqa_samples, n_img_tokens=n_img_tokens\n",
    ")\n",
    "stacked_attn_sums.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [0, 15, 25]\n",
    "stacked_attens = stacked_attn_sums\n",
    "means = stacked_attens.mean(dim=0)\n",
    "assert means.shape == (len(layers), 4, 4)\n",
    "stds = stacked_attens.std(dim=0)\n",
    "assert stds.shape == (len(layers), 4, 4)\n",
    "fig = plot_mult_attn_sums(None, None, layers=layers, mult_attn_sums=means, stds=stds)\n",
    "fig.savefig(f\"imgs/blockwise-attn-sums-noisy-vqa{n_vqa_samples}.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check:\n",
    "# In layer 0, the image tokens attend to other image tokens with no less than 93% of all attention (is very concentrated there)\n",
    "stacked_attens[:, 0, 0, 0].min()  # success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use different image as corrupted input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frisbee2_img_path = \"imgs/frisbee2.png\"\n",
    "frisbee2_img = Image.open(frisbee2_img_path)\n",
    "plt.axis(\"off\")\n",
    "_ = plt.imshow(frisbee2_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomas: I thought the frisbee was white, but LLMs disagree.\n",
    "# ChatGPT: \"The frisbee in the image is light blue.\"\n",
    "# Gemini: \"The frisbee's color is light blue.\"\n",
    "# Claude: \"The frisbee is light blue.\"\n",
    "print(get_response(model, processor, text, frisbee2_img)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_token = processor.tokenizer.encode(\"blue\")\n",
    "assert processor.tokenizer.decode(blue_token) == \"blue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "frisbee2_inputs = processor(text=text, images=frisbee2_img, return_tensors=\"pt\").to(\n",
    "    model.device\n",
    ")\n",
    "frisbee2_embeds = paligemma_merge_text_and_image(model, frisbee2_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import ActivationPatchingResult, patch_all_activations\n",
    "\n",
    "\n",
    "frisbee2_patching_result: ActivationPatchingResult = patch_all_activations(\n",
    "    model,\n",
    "    healthy_activations,\n",
    "    frisbee2_embeds,\n",
    "    healthy_response_tok_idx=purple_token,\n",
    "    unhealthy_response_tok_idx=blue_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "frisbee2_patching_result.save(\n",
    "    directory=Path(\"patching_result_str\"),\n",
    "    health_response_tok=\"purple\",\n",
    "    unhealthy_response_tok=\"blue\",\n",
    "    corruption_type=\"token_replacement\",\n",
    "    corruption_img_alias=frisbee2_img_path,\n",
    "    healthy_img_alias=img_path,\n",
    "    prompt=text,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import plot_mult_attn_sums\n",
    "\n",
    "plot_mult_attn_sums(\n",
    "    model, {\"inputs_embeds\": frisbee2_embeds}, layers=[0, 15, 25], n_img_tokens=256\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import (\n",
    "    plot_pooled_probs_plotly,\n",
    "    maxpool_img_tokens,\n",
    "    plot_img_and_text_probs_side_by_side,\n",
    "    plot_pooled_probs_plt,\n",
    "    plot_and_browse_img_token_in_probs,\n",
    ")\n",
    "\n",
    "frisbee2_purple_probs = ActivationPatchingResult.load(\n",
    "    Path(\"patching_result_str\")\n",
    ").healthy_tok_response_probs\n",
    "# frisbee2_pooled_purple_probs = maxpool_img_tokens(frisbee2_purple_probs, n_img_tokens)\n",
    "# plot_and_browse_img_token_in_purple_probs(purple_probs, inputs_tokens)\n",
    "# plot_pooled_purple_probs_plt(dino_pooled_purple_probs, inputs_tokens).show()\n",
    "plot_img_and_text_probs_side_by_side(\n",
    "    frisbee2_purple_probs, n_img_tokens, inputs_tokens\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does the `<bos>` token have no outflowing information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<image>Answer en what color is the frisbee?\"\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(model.device)\n",
    "# add to transformers/models/gemma2/modeling_gemma2.py line 402 (forward right before attention)\n",
    "# ```python\n",
    "# from pathlib import Path\n",
    "# for i in range(1, 30):\n",
    "#     fname = f\"value_states/layer{i}.pt\"\n",
    "#     if not Path(fname).exists():\n",
    "#         torch.save(value_states, fname)\n",
    "#         print(f\"Saved {fname}\")\n",
    "#         break\n",
    "# ```\n",
    "# model(**inputs)\n",
    "inputs_tokens = processor.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "inputs_tokens[-1] = \"\\\\n\"\n",
    "n_img_tokens = 256\n",
    "print(inputs_tokens[n_img_tokens:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl1 = torch.load(\"value_states/layer1.pt\")\n",
    "vsl26 = torch.load(\"value_states/layer26.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(vsl1, vsl1)\n",
    "assert torch.allclose(vsl26, vsl26)\n",
    "assert not torch.allclose(vsl1, vsl26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = (\n",
    "    torch.norm(vsl1[0, 0, :, :], dim=1).cpu().float()\n",
    ")  # layer1, batch example 0, head 0, all tokens, all features\n",
    "img_norms = norms[:n_img_tokens]\n",
    "img_norms.mean(), img_norms.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_norms = norms[n_img_tokens:]\n",
    "txt_norms.mean(), txt_norms.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsls = [torch.load(f\"value_states/layer{i}.pt\") for i in range(1, 27)]\n",
    "assert len(vsls) == 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the batch dimension, compute norms in each head\n",
    "all_norms = torch.norm(torch.stack(vsls).squeeze(1), dim=-1)\n",
    "all_norms = all_norms.cpu().float()\n",
    "all_norms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_norms = all_norms[:, :, n_img_tokens:].mean(dim=1)  # average over all heads\n",
    "print(all_text_norms.shape)\n",
    "plt.imshow(\n",
    "    all_text_norms.T, cmap=\"Blues\", vmin=0, vmax=all_text_norms.max()\n",
    ")  # only text tokens accross all layers\n",
    "plt.colorbar()\n",
    "plt.yticks(ticks=range(len(txt_norms)), labels=inputs_tokens[n_img_tokens:])\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Logit Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import ActivationPatchingResult, plot_img_and_text_probs_side_by_side\n",
    "from pathlib import Path\n",
    "\n",
    "pr = ActivationPatchingResult.load(Path(\"patching_result_str\"))\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(model.device)\n",
    "inputs_tokens = [processor.decode(id) for id in inputs.input_ids[0]]\n",
    "logit_diff = pr.healthy_tok_response_logits - pr.unhealthy_tok_response_logits\n",
    "plot_img_and_text_probs_side_by_side(logit_diff, n_img_tokens=256, token_strings=inputs_tokens).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
