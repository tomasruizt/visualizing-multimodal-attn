{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from transformers import (\n",
    "    PaliGemmaProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    "    Gemma2Model,\n",
    "    Gemma2ForCausalLM,\n",
    ")\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_grad_enabled(False)  # avoid blowing up mem\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/paligemma2-3b-pt-224\"\n",
    "model = (\n",
    "    PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "        model_id, torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    .to(device)\n",
    "    .eval()\n",
    ")\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"imgs/frisbee.jpg\"\n",
    "image = Image.open(img_path)\n",
    "plt.axis(\"off\")\n",
    "_ = plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import get_response\n",
    "\n",
    "\n",
    "text = \"<image>Answer en what is the frisbee's color?\"\n",
    "inputs_tokens, response = get_response(model, processor, text, image)\n",
    "inputs_tokens[-1] = \"\\\\n\"  # to print it nicely\n",
    "print(\"len(inputs_tokens) =\", len(inputs_tokens))\n",
    "print(inputs_tokens)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(model.device)\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import (\n",
    "    Hook,\n",
    "    State,\n",
    "    get_activations,\n",
    "    paligemma_merge_text_and_image,\n",
    "    gaussian_noising,\n",
    ")\n",
    "\n",
    "healthy_embeds = paligemma_merge_text_and_image(model, inputs)\n",
    "healthy_activations, healthy_outputs = get_activations(model, healthy_embeds)\n",
    "print(processor.decode(healthy_outputs.logits[0, -1, :].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_embeds = gaussian_noising(healthy_embeds, num_img_tokens=256)\n",
    "jammed_activations, jammed_outputs = get_activations(model, unhealthy_embeds)\n",
    "print(processor.decode(jammed_outputs.logits[0, -1, :].argmax()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import tok_prob\n",
    "\n",
    "purple_token = 34999\n",
    "assert processor.tokenizer.decode(purple_token) == \"purple\"\n",
    "assert jammed_activations.shape == healthy_activations.shape\n",
    "assert (\n",
    "    tok_prob(healthy_outputs, purple_token)\n",
    "    == model(**inputs).logits[0, -1, :].softmax(dim=-1).max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from getAttentionLib import loop_over_restore_all_activations\n",
    "\n",
    "\n",
    "# purple_probs = loop_over_restore_all_activations(\n",
    "#     model, healthy_activations, unhealthy_embeds, healthy_response_tok_idx=purple_token\n",
    "# )\n",
    "# torch.save(purple_probs, \"purple_probs_of_noisy_frisbee_img.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "purple_probs = torch.load(\"purple_probs_of_noisy_frisbee_img.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does the `<bos>` token get attention?\n",
    "As we can see below, the noisy input embeddings have a significantly different attention pattern than the healthy image input. \n",
    "Almost all attention goes in the image tokens. Almost no attention is allocated to the the `<bos>` token, which explains why it cannot influence the output, even when restored.\n",
    "This suggests that noise input data disrupts the learned attention patterns. To preserve the attention patterns, we need a different approach to corrput the input image.\n",
    "Using a different, unrelated image might work better, because it will preserve the attention patterns, as shown in VQA example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import maxpool_img_tokens, plot_pooled_probs_plotly\n",
    "\n",
    "\n",
    "plot_pooled_probs_plotly(maxpool_img_tokens(purple_probs), inputs_tokens, healthy_response_tok_name=\"purple\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import (\n",
    "    compute_mult_attn_sums,\n",
    "    plot_region_attn_progression,\n",
    "    plot_mult_attn_sums,\n",
    ")\n",
    "\n",
    "mult_attn_sums = compute_mult_attn_sums(\n",
    "    model,\n",
    "    {\"inputs_embeds\": unhealthy_embeds},\n",
    "    layers=list(range(len(model.language_model.model.layers))),\n",
    "    n_img_tokens=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_mult_attn_sums(\n",
    "#     None, None, layers=[0, 15, 25], mult_attn_sums=mult_attn_sums\n",
    "# ).show()\n",
    "# plot_region_attn_progression(None, None, mult_attn_sums=mult_attn_sums).show()\n",
    "plot_mult_attn_sums(None, None, layers=[0, 15, 25], mult_attn_sums=[mult_attn_sums[e] for e in [0, 15, 25]]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use different image as corrupted input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frisbee2_img = Image.open(\"imgs/frisbee2.png\")\n",
    "plt.axis(\"off\")\n",
    "_ = plt.imshow(frisbee2_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomas: I thought the frisbee was white, but LLMs disagree.\n",
    "# ChatGPT: \"The frisbee in the image is light blue.\"\n",
    "# Gemini: \"The frisbee's color is light blue.\"\n",
    "# Claude: \"The frisbee is light blue.\"\n",
    "print(get_response(model, processor, text, frisbee2_img)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frisbee2_inputs = processor(text=text, images=frisbee2_img, return_tensors=\"pt\").to(\n",
    "    model.device\n",
    ")\n",
    "frisbee2_embeds = paligemma_merge_text_and_image(model, frisbee2_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import plot_mult_attn_sums\n",
    "\n",
    "plot_mult_attn_sums(model, {\"inputs_embeds\": frisbee2_embeds}, layers=[0, 15, 25], n_img_tokens=256).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from getAttentionLib import loop_over_restore_all_activations\n",
    "\n",
    "\n",
    "# purple_probs = loop_over_restore_all_activations(\n",
    "#     model, healthy_activations, unhealthy_embeds=frisbee2_embeds, healthy_response_tok_idx=purple_token\n",
    "# )\n",
    "# torch.save(purple_probs, \"purple_probs_of_frisbee2_img.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import plot_pooled_probs_plotly\n",
    "from getAttentionLib import plot_pooled_probs_plt\n",
    "from getAttentionLib import plot_and_browse_img_token_in_probs\n",
    "\n",
    "frisbee2_purple_probs = torch.load(\"purple_probs_of_frisbee2_img.pt\")\n",
    "frisbee2_pooled_purple_probs = maxpool_img_tokens(frisbee2_purple_probs)\n",
    "# plot_and_browse_img_token_in_purple_probs(purple_probs, inputs_tokens)\n",
    "# plot_pooled_purple_probs_plt(dino_pooled_purple_probs, inputs_tokens).show()\n",
    "plot_pooled_probs_plotly(frisbee2_pooled_purple_probs, inputs_tokens, healthy_response_tok_name=\"purple\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does the `<bos>` token have no outflowing information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<image>Answer en what color is the frisbee?\"\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(model.device)\n",
    "# add to transformers/models/gemma2/modeling_gemma2.py line 402 (forward right before attention)\n",
    "# ```python\n",
    "# from pathlib import Path\n",
    "# for i in range(1, 30):\n",
    "#     fname = f\"value_states/layer{i}.pt\"\n",
    "#     if not Path(fname).exists():\n",
    "#         torch.save(value_states, fname)\n",
    "#         print(f\"Saved {fname}\")\n",
    "#         break\n",
    "# ```\n",
    "# model(**inputs)\n",
    "inputs_tokens = processor.tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "inputs_tokens[-1] = \"\\\\n\"\n",
    "n_img_tokens = 256\n",
    "print(inputs_tokens[n_img_tokens:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl1 = torch.load(\"value_states/layer1.pt\")\n",
    "vsl26 = torch.load(\"value_states/layer26.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(vsl1, vsl1)\n",
    "assert torch.allclose(vsl26, vsl26)\n",
    "assert not torch.allclose(vsl1, vsl26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = torch.norm(vsl1[0,0,:,:], dim=1).cpu().float() # layer1, batch example 0, head 0, all tokens, all features\n",
    "img_norms = norms[:n_img_tokens]\n",
    "img_norms.mean(), img_norms.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_norms = norms[n_img_tokens:]\n",
    "txt_norms.mean(), txt_norms.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsls = [torch.load(f\"value_states/layer{i}.pt\") for i in range(1, 27)]\n",
    "assert len(vsls) == 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the batch dimension, compute norms in each head\n",
    "all_norms = torch.norm(torch.stack(vsls).squeeze(1), dim=-1)\n",
    "all_norms = all_norms.cpu().float()\n",
    "all_norms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_norms = all_norms[:, :, n_img_tokens:].mean(dim=1) # average over all heads\n",
    "print(all_text_norms.shape)\n",
    "plt.imshow(all_text_norms.T, cmap=\"Blues\", vmin=0, vmax=all_text_norms.max()) # only text tokens accross all layers\n",
    "plt.colorbar()\n",
    "plt.yticks(ticks=range(len(txt_norms)), labels=inputs_tokens[n_img_tokens:])\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
