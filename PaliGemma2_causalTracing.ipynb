{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from transformers import (\n",
    "    PaliGemmaProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    "    Gemma2Model,\n",
    "    Gemma2ForCausalLM,\n",
    ")\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_grad_enabled(False)  # avoid blowing up mem\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/paligemma2-3b-pt-224\"\n",
    "model = (\n",
    "    PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "        model_id, torch_dtype=torch.bfloat16\n",
    "    )\n",
    "    .to(device)\n",
    "    .eval()\n",
    ")\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"imgs/frisbee.jpg\"\n",
    "image = Image.open(img_path)\n",
    "plt.axis(\"off\")\n",
    "_ = plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import get_response\n",
    "\n",
    "\n",
    "text = \"<image>Answer en what is the frisbee's color?\"\n",
    "inputs_tokens, response = get_response(model, processor, text, image)\n",
    "inputs_tokens[-1] = \"\\\\n\"  # to print it nicely\n",
    "print(\"len(inputs_tokens) =\", len(inputs_tokens))\n",
    "print(inputs_tokens)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_idxs = (len(inputs_tokens) - 8, len(inputs_tokens) - 2)\n",
    "print(object_idxs)\n",
    "print(inputs_tokens[object_idxs[0] : object_idxs[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=text, images=image, return_tensors=\"pt\").to(model.device)\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import (\n",
    "    Hook,\n",
    "    State,\n",
    "    get_activations,\n",
    "    paligemma_merge_text_and_image,\n",
    "    jam_img_embeds,\n",
    ")\n",
    "\n",
    "healthy_embeds = paligemma_merge_text_and_image(model, inputs)\n",
    "healthy_activations, healthy_outputs = get_activations(model, healthy_embeds)\n",
    "print(processor.decode(healthy_outputs.logits[0, -1, :].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy_embeds = jam_img_embeds(healthy_embeds, num_img_tokens=256)\n",
    "jammed_activations, jammed_outputs = get_activations(model, unhealthy_embeds)\n",
    "print(processor.decode(jammed_outputs.logits[0, -1, :].argmax()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import tok_prob\n",
    "\n",
    "purple_token = 34999\n",
    "assert processor.tokenizer.decode(purple_token) == \"purple\"\n",
    "assert jammed_activations.shape == healthy_activations.shape\n",
    "assert (\n",
    "    tok_prob(healthy_outputs, purple_token)\n",
    "    == model(**inputs).logits[0, -1, :].softmax(dim=-1).max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from getAttentionLib import loop_over_restore_all_activations\n",
    "\n",
    "\n",
    "# purple_probs = loop_over_restore_all_activations(\n",
    "#     model, healthy_activations, unhealthy_embeds, healthy_response_tok_idx=purple_token\n",
    "# )\n",
    "# torch.save(purple_probs, \"purple_probs_of_noisy_frisbee_img.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "purple_probs = torch.load(\"purple_probs_of_noisy_frisbee_img.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does the `<bos>` token get attention?\n",
    "As we can see below, the noisy input embeddings have a significantly different attention pattern than the healthy image input. \n",
    "Almost all attention goes in the image tokens. Almost no attention is allocated to the the `<bos>` token, which explains why it cannot influence the output, even when restored.\n",
    "This suggests that noise input data disrupts the learned attention patterns. To preserve the attention patterns, we need a different approach to corrput the input image.\n",
    "Using a different, unrelated image might work better, because it will preserve the attention patterns, as shown in VQA example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import maxpool_img_tokens, plot_pooled_probs_plotly\n",
    "\n",
    "\n",
    "plot_pooled_probs_plotly(maxpool_img_tokens(purple_probs), inputs_tokens).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import (\n",
    "    compute_mult_attn_sums,\n",
    "    plot_region_attn_progression,\n",
    "    plot_mult_attn_sums,\n",
    ")\n",
    "\n",
    "mult_attn_sums = compute_mult_attn_sums(\n",
    "    model,\n",
    "    {\"inputs_embeds\": unhealthy_embeds},\n",
    "    layers=list(range(len(model.language_model.model.layers))),\n",
    "    n_img_tokens=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_mult_attn_sums(\n",
    "#     None, None, layers=[0, 15, 25], mult_attn_sums=mult_attn_sums\n",
    "# ).show()\n",
    "# plot_region_attn_progression(None, None, mult_attn_sums=mult_attn_sums).show()\n",
    "plot_mult_attn_sums(None, None, layers=[0, 15, 25], mult_attn_sums=[mult_attn_sums[e] for e in [0, 15, 25]]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use different image as corrupted input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frisbee2_img = Image.open(\"imgs/frisbee2.png\")\n",
    "plt.axis(\"off\")\n",
    "_ = plt.imshow(frisbee2_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomas: I thought the frisbee was white, but LLMs disagree.\n",
    "# ChatGPT: \"The frisbee in the image is light blue.\"\n",
    "# Gemini: \"The frisbee's color is light blue.\"\n",
    "# Claude: \"The frisbee is light blue.\"\n",
    "print(get_response(model, processor, text, frisbee2_img)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "frisbee2_inputs = processor(text=text, images=frisbee2_img, return_tensors=\"pt\").to(\n",
    "    model.device\n",
    ")\n",
    "frisbee2_embeds = paligemma_merge_text_and_image(model, frisbee2_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import plot_mult_attn_sums\n",
    "\n",
    "plot_mult_attn_sums(model, {\"inputs_embeds\": frisbee2_embeds}, layers=[0, 15, 25], n_img_tokens=256).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from getAttentionLib import loop_over_restore_all_activations\n",
    "\n",
    "\n",
    "# purple_probs = loop_over_restore_all_activations(\n",
    "#     model, healthy_activations, unhealthy_embeds=frisbee2_embeds, healthy_response_tok_idx=purple_token\n",
    "# )\n",
    "# torch.save(purple_probs, \"purple_probs_of_frisbee2_img.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getAttentionLib import plot_pooled_probs_plotly\n",
    "from getAttentionLib import plot_pooled_probs_plt\n",
    "from getAttentionLib import plot_and_browse_img_token_in_probs\n",
    "\n",
    "frisbee2_purple_probs = torch.load(\"purple_probs_of_frisbee2_img.pt\")\n",
    "frisbee2_pooled_purple_probs = maxpool_img_tokens(frisbee2_purple_probs)\n",
    "# plot_and_browse_img_token_in_purple_probs(purple_probs, inputs_tokens)\n",
    "# plot_pooled_purple_probs_plt(dino_pooled_purple_probs, inputs_tokens).show()\n",
    "plot_pooled_probs_plotly(frisbee2_pooled_purple_probs, inputs_tokens, healthy_response_tok_name=\"purple\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
